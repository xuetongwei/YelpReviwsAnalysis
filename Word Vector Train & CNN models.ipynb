{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from numpy.random import shuffle\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty standard Chinese food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Never had a bad experience here--it tastes j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When we sat down i had one thing on my mind \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But it was okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What really made my night was the STUPENDOUS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews\n",
       "0                       Pretty standard Chinese food\n",
       "1    Never had a bad experience here--it tastes j...\n",
       "2   When we sat down i had one thing on my mind \"...\n",
       "3                                    But it was okay\n",
       "4   What really made my night was the STUPENDOUS ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load reviews for wordvector training\n",
    "train=pd.read_csv(\"wordvector_train.csv\",header=0,delimiter=\"\\t\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bid</td>\n",
       "      <td>reviews</td>\n",
       "      <td>Cat</td>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iFEiMJoEqyB9O8OUNSdLzA</td>\n",
       "      <td>Pretty standard Chinese food.  Never had a bad...</td>\n",
       "      <td>['Chinese', 'Restaurants']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HmI9nhgOkrXlUr6KZGZZew</td>\n",
       "      <td>This little diner is a staple in Bloomfield's ...</td>\n",
       "      <td>['Sandwiches', 'Restaurants', 'Italian', 'Dine...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLTbKRmBytP1AbjWSPEeyw</td>\n",
       "      <td>Ever since I had one of the best meals of my l...</td>\n",
       "      <td>['Restaurants', 'Chinese']</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58APdML-PG_OD4El2ePTvw</td>\n",
       "      <td>Location: Tucked away in the Old Port. The clo...</td>\n",
       "      <td>['Restaurants', 'Specialty Food', 'French', 'B...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0                                                  1  \\\n",
       "0                     bid                                            reviews   \n",
       "1  iFEiMJoEqyB9O8OUNSdLzA  Pretty standard Chinese food.  Never had a bad...   \n",
       "2  HmI9nhgOkrXlUr6KZGZZew  This little diner is a staple in Bloomfield's ...   \n",
       "3  FLTbKRmBytP1AbjWSPEeyw  Ever since I had one of the best meals of my l...   \n",
       "4  58APdML-PG_OD4El2ePTvw  Location: Tucked away in the Old Port. The clo...   \n",
       "\n",
       "                                                   2     3  \n",
       "0                                                Cat  star  \n",
       "1                         ['Chinese', 'Restaurants']     3  \n",
       "2  ['Sandwiches', 'Restaurants', 'Italian', 'Dine...     3  \n",
       "3                         ['Restaurants', 'Chinese']     4  \n",
       "4  ['Restaurants', 'Specialty Food', 'French', 'B...     4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load parsed dataset for aspect and sentiment prediction\n",
    "predict=pd.read_csv(\"data_predict.csv\",header=-1,delimiter=\",\")\n",
    "predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty standard Chinese food</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Never had a bad experience here--it tastes j...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When we sat down i had one thing on my mind \"...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But it was okay</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What really made my night was the STUPENDOUS ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Aspects\n",
       "0                       Pretty standard Chinese food    food\n",
       "1    Never had a bad experience here--it tastes j...    food\n",
       "2   When we sat down i had one thing on my mind \"...    food\n",
       "3                                    But it was okay    food\n",
       "4   What really made my night was the STUPENDOUS ...    food"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load labelled aspect reviews for training \n",
    "aspect_l=pd.read_csv(\"labelled1.csv\", header=0, delimiter=\",\")\n",
    "aspect_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty standard Chinese food</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Never had a bad experience here--it tastes j...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When we sat down i had one thing on my mind \"...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'll take the tortilla chips and cheesy salsa...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But it was okay</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews sentiment\n",
       "0                       Pretty standard Chinese food   Neutral\n",
       "1    Never had a bad experience here--it tastes j...  Positive\n",
       "2   When we sat down i had one thing on my mind \"...  Negative\n",
       "3   I'll take the tortilla chips and cheesy salsa...   Neutral\n",
       "4                                    But it was okay   Neutral"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load labelled aspect reviews for training \n",
    "sentiment_l=pd.read_csv(\"labelled3.csv\", header=0, delimiter=\",\")\n",
    "sentiment_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample label of labelled aspects\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]]\n",
      "sample label of labelled sentiments\n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#convert labels into the form of \"list of list\"\n",
    "labels1=[str(item[\"Aspects\"]).split(\",\") for index, item in aspect_l.iterrows()]\n",
    "labels2=[str(item[\"sentiment\"]).split(\",\") for index, item in sentiment_l.iterrows()]\n",
    "#create indicator matrix for labels\n",
    "mlb1=MultiLabelBinarizer()\n",
    "mlb2=MultiLabelBinarizer()\n",
    "Y1=mlb1.fit_transform(labels1)\n",
    "Y2=mlb2.fit_transform(labels2)\n",
    "print(\"sample label of labelled aspects\")\n",
    "print(Y1[0:5])\n",
    "print(\"sample label of labelled sentiments\")\n",
    "print(Y2[0:5])\n",
    "#mlb1.classes_\n",
    "#mlb2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pretty', 'standard', 'Chinese', 'food'], ['Never', 'had', 'bad', 'experience', 'here', 'it', 'tastes', 'just', 'like', 'you', 'expect', 'take', 'out', 'Chinese', 'to', 'taste']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize reviews in wordvector_train.csv and prepare for word vector training\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in train['Reviews']]\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-03 17:15:19,265 : INFO : collecting all words and their counts\n",
      "2017-12-03 17:15:19,267 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-03 17:15:19,610 : INFO : PROGRESS: at sentence #10000, processed 960863 words, keeping 45009 word types\n",
      "2017-12-03 17:15:19,937 : INFO : PROGRESS: at sentence #20000, processed 1949496 words, keeping 69358 word types\n",
      "2017-12-03 17:15:20,271 : INFO : PROGRESS: at sentence #30000, processed 2930472 words, keeping 89602 word types\n",
      "2017-12-03 17:15:20,659 : INFO : PROGRESS: at sentence #40000, processed 3932435 words, keeping 107932 word types\n",
      "2017-12-03 17:15:20,716 : INFO : collected 110372 word types from a corpus of 4072390 raw words and 41447 sentences\n",
      "2017-12-03 17:15:20,717 : INFO : Loading a fresh vocabulary\n",
      "2017-12-03 17:15:21,030 : INFO : min_count=5 retains 21539 unique words (19% of original 110372, drops 88833)\n",
      "2017-12-03 17:15:21,032 : INFO : min_count=5 leaves 3948389 word corpus (96% of original 4072390, drops 124001)\n",
      "2017-12-03 17:15:21,137 : INFO : deleting the raw counts dictionary of 110372 items\n",
      "2017-12-03 17:15:21,143 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2017-12-03 17:15:21,145 : INFO : downsampling leaves estimated 3105119 word corpus (78.6% of prior 3948389)\n",
      "2017-12-03 17:15:21,147 : INFO : estimated required memory for 21539 words and 100 dimensions: 28000700 bytes\n",
      "2017-12-03 17:15:21,266 : INFO : resetting layer weights\n",
      "2017-12-03 17:15:21,801 : INFO : training model with 4 workers on 21539 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-03 17:15:22,821 : INFO : PROGRESS: at 3.68% examples, 547769 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:23,827 : INFO : PROGRESS: at 8.15% examples, 623141 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-03 17:15:24,832 : INFO : PROGRESS: at 12.31% examples, 629034 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:25,833 : INFO : PROGRESS: at 15.67% examples, 602458 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:26,841 : INFO : PROGRESS: at 18.60% examples, 574711 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:27,843 : INFO : PROGRESS: at 22.23% examples, 569658 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:28,848 : INFO : PROGRESS: at 26.16% examples, 575333 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:29,855 : INFO : PROGRESS: at 30.37% examples, 585050 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-03 17:15:30,863 : INFO : PROGRESS: at 34.75% examples, 594849 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:31,864 : INFO : PROGRESS: at 38.71% examples, 597990 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:32,874 : INFO : PROGRESS: at 42.95% examples, 601499 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-03 17:15:33,876 : INFO : PROGRESS: at 46.96% examples, 603514 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:34,880 : INFO : PROGRESS: at 50.89% examples, 603948 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:35,891 : INFO : PROGRESS: at 55.04% examples, 606163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:36,896 : INFO : PROGRESS: at 58.66% examples, 603876 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:37,912 : INFO : PROGRESS: at 62.91% examples, 605664 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:38,926 : INFO : PROGRESS: at 67.17% examples, 608667 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-03 17:15:39,939 : INFO : PROGRESS: at 71.58% examples, 612561 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-03 17:15:40,943 : INFO : PROGRESS: at 75.63% examples, 613204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:41,957 : INFO : PROGRESS: at 78.71% examples, 606645 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:42,965 : INFO : PROGRESS: at 81.79% examples, 599536 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:43,986 : INFO : PROGRESS: at 85.80% examples, 600239 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-03 17:15:45,002 : INFO : PROGRESS: at 89.84% examples, 601005 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-03 17:15:46,006 : INFO : PROGRESS: at 94.37% examples, 605037 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:47,012 : INFO : PROGRESS: at 98.52% examples, 606971 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-03 17:15:47,347 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-03 17:15:47,350 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-03 17:15:47,352 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-03 17:15:47,362 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-03 17:15:47,363 : INFO : training on 20361950 raw words (15525431 effective words) took 25.5s, 607699 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#word vector training process\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "EMBEDDING_DIM=100\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: is the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, \\\n",
    "                             size=EMBEDDING_DIM, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# get word vector for all words in the vocabulary\n",
    "\n",
    "MAX_NB_WORDS=2000\n",
    "\n",
    "# tokenizer.word_index provides the mapping \n",
    "# between a word and word index for all words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train[\"Reviews\"])\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "\n",
    "# \"+1\" is for padding symbol\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # if word_index is above the max number of words, ignore it\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#cnn traning model function\n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              EMBEDDING_DIM=100, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.01):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z=Concatenate(name='concate')(conv_blocks)\n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(192, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model\n",
    "\n",
    "#name best models\n",
    "BEST_MODEL_FILEPATH1=\"best_model1\"\n",
    "BEST_MODEL_FILEPATH2=\"best_model2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69864\n",
      "[('pretty', 5383), ('standard', 759), ('chinese', 2410), ('food', 27086), ('never', 5179)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLOVQMDOzlEPBzMxSDgUzM0v9f7gaaFNNPCMvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23fbc997da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define MAX_NB_WORDS  \n",
    "# Set MAX_NB_WORDS to include words that appear at least K times\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# total number of words\n",
    "total_nb_words=len(tokenizer.word_counts)\n",
    "print(total_nb_words)\n",
    "\n",
    "print(list(tokenizer.word_counts.items())[0:5])\n",
    "# put word and its counts into a data frame\n",
    "word_counts=pd.DataFrame(\\\n",
    "            list(tokenizer.word_counts.items()), \\\n",
    "            columns=['word','count'])\n",
    "word_counts.head(3)\n",
    "\n",
    "# get histogram of word counts\n",
    "# after reset index, \"index\" column \n",
    "# is the word frequency\n",
    "# \"count\" column gives how many words appear at \n",
    "# a specific frequency\n",
    "df=word_counts['count'].value_counts().reset_index()\n",
    "df.head(3)\n",
    "\n",
    "# convert absolute counts to precentage\n",
    "df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "# get cumulative percentage\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "df.head(5)\n",
    "\n",
    "# plot the chart\n",
    "# then we decided to set MAX_NB_WORDS=2000\n",
    "plt.bar(df[\"index\"].iloc[0:50], df[\"percent\"].iloc[0:50])\n",
    "plt.plot(df[\"index\"].iloc[0:50], df['cumsum'].iloc[0:50], c='green')\n",
    "\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nurVmy7OCDBUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23fbc9d1630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define MAX_DOC_LEN \n",
    "# Set MAX_DOC_LEN to include complete sentences as many as possible\n",
    "\n",
    "# create a series based on the length of all sentences\n",
    "sen_len=pd.Series([len(item) for item in sequences_aspect])\n",
    "\n",
    "# create histogram of sentence length\n",
    "# the \"index\" is the sentence length\n",
    "# \"counts\" is the count of sentences at a length\n",
    "df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "df.columns=['index','counts']\n",
    "df.head(3)\n",
    "\n",
    "# sort by sentence length\n",
    "# get percentage and cumulative percentage\n",
    "\n",
    "df=df.sort_values(by='index')\n",
    "df['percent']=df['counts']/len(sen_len)\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "df.head(3)\n",
    "\n",
    "# From the plot, 90% sentences have length<30, and for both aspect and sentiment dataset the plot performence is alike\n",
    "# so we decided to set MAX_DOC_LEN=30 \n",
    "plt.plot(df[\"index\"], df['cumsum'], c='green')\n",
    "\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280 samples, validate on 120 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.78542, saving model to best_model1\n",
      "1s - loss: 2.9609 - acc: 0.6464 - val_loss: 2.6193 - val_acc: 0.7854\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.78542 to 0.79167, saving model to best_model1\n",
      "0s - loss: 2.6174 - acc: 0.7571 - val_loss: 2.5070 - val_acc: 0.7917\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.79167 to 0.80417, saving model to best_model1\n",
      "0s - loss: 2.4190 - acc: 0.7723 - val_loss: 2.3541 - val_acc: 0.8042\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc improved from 0.80417 to 0.81458, saving model to best_model1\n",
      "0s - loss: 2.2813 - acc: 0.7946 - val_loss: 2.2320 - val_acc: 0.8146\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.81458 to 0.82500, saving model to best_model1\n",
      "0s - loss: 2.1972 - acc: 0.8054 - val_loss: 2.0921 - val_acc: 0.8250\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc improved from 0.82500 to 0.83333, saving model to best_model1\n",
      "0s - loss: 2.0776 - acc: 0.8214 - val_loss: 2.0241 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc did not improve\n",
      "0s - loss: 2.0026 - acc: 0.8286 - val_loss: 1.9843 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc did not improve\n",
      "0s - loss: 1.9077 - acc: 0.8491 - val_loss: 1.9272 - val_acc: 0.8313\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.83333 to 0.83542, saving model to best_model1\n",
      "0s - loss: 1.8422 - acc: 0.8509 - val_loss: 1.8528 - val_acc: 0.8354\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc improved from 0.83542 to 0.84167, saving model to best_model1\n",
      "0s - loss: 1.7725 - acc: 0.8598 - val_loss: 1.7872 - val_acc: 0.8417\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "0s - loss: 1.6850 - acc: 0.8804 - val_loss: 1.7425 - val_acc: 0.8292\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "0s - loss: 1.6422 - acc: 0.8741 - val_loss: 1.6860 - val_acc: 0.8417\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc improved from 0.84167 to 0.85000, saving model to best_model1\n",
      "0s - loss: 1.5528 - acc: 0.8920 - val_loss: 1.6301 - val_acc: 0.8500\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc improved from 0.85000 to 0.85417, saving model to best_model1\n",
      "0s - loss: 1.5123 - acc: 0.8902 - val_loss: 1.5754 - val_acc: 0.8542\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc did not improve\n",
      "0s - loss: 1.4333 - acc: 0.9089 - val_loss: 1.5417 - val_acc: 0.8438\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "0s - loss: 1.3843 - acc: 0.9116 - val_loss: 1.4933 - val_acc: 0.8458\n",
      "Epoch 17/100\n",
      "Epoch 00016: val_acc did not improve\n",
      "0s - loss: 1.3159 - acc: 0.9268 - val_loss: 1.4456 - val_acc: 0.8521\n",
      "Epoch 18/100\n",
      "Epoch 00017: val_acc did not improve\n",
      "0s - loss: 1.2783 - acc: 0.9321 - val_loss: 1.4164 - val_acc: 0.8500\n",
      "Epoch 19/100\n",
      "Epoch 00018: val_acc did not improve\n",
      "0s - loss: 1.2182 - acc: 0.9304 - val_loss: 1.3639 - val_acc: 0.8458\n",
      "Epoch 20/100\n",
      "Epoch 00019: val_acc improved from 0.85417 to 0.85833, saving model to best_model1\n",
      "0s - loss: 1.1719 - acc: 0.9321 - val_loss: 1.3197 - val_acc: 0.8583\n",
      "Epoch 21/100\n",
      "Epoch 00020: val_acc did not improve\n",
      "0s - loss: 1.1203 - acc: 0.9339 - val_loss: 1.3026 - val_acc: 0.8500\n",
      "Epoch 22/100\n",
      "Epoch 00021: val_acc improved from 0.85833 to 0.86042, saving model to best_model1\n",
      "0s - loss: 1.0684 - acc: 0.9473 - val_loss: 1.2540 - val_acc: 0.8604\n",
      "Epoch 23/100\n",
      "Epoch 00022: val_acc did not improve\n",
      "0s - loss: 1.0524 - acc: 0.9366 - val_loss: 1.2215 - val_acc: 0.8583\n",
      "Epoch 24/100\n",
      "Epoch 00023: val_acc improved from 0.86042 to 0.86250, saving model to best_model1\n",
      "0s - loss: 0.9911 - acc: 0.9527 - val_loss: 1.1893 - val_acc: 0.8625\n",
      "Epoch 25/100\n",
      "Epoch 00024: val_acc improved from 0.86250 to 0.87292, saving model to best_model1\n",
      "0s - loss: 0.9574 - acc: 0.9527 - val_loss: 1.1395 - val_acc: 0.8729\n",
      "Epoch 26/100\n",
      "Epoch 00025: val_acc did not improve\n",
      "0s - loss: 0.9122 - acc: 0.9562 - val_loss: 1.1552 - val_acc: 0.8625\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fit aspect model using pretrained word vectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set the number of output units\n",
    "NUM_OUTPUT_UNITS=len(mlb1.classes_)\n",
    "\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# We used 400 labeled aspect reviews\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "MAX_DOC_LEN=30\n",
    "sequences_aspect = tokenizer.texts_to_sequences(aspect_l[\"Reviews\"])\n",
    "padded_sequences_aspect = pad_sequences(sequences_aspect, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "#\n",
    "sequences_1 = tokenizer.texts_to_sequences(predict[1])\n",
    "padded_sequences_1 = pad_sequences(sequences_1, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences_aspect[0:400], Y1[0:400], \\\n",
    "                test_size=0.3, random_state=0)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, NUM_OUTPUT_UNITS, \\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH1, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 94.27%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of aspect CNN model\n",
    "scores_1 = model.evaluate(padded_sequences_aspect, Y1, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 314 samples, validate on 135 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.66667, saving model to best_model2\n",
      "1s - loss: 2.7983 - acc: 0.6401 - val_loss: 2.5048 - val_acc: 0.6667\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.66667 to 0.75556, saving model to best_model2\n",
      "0s - loss: 2.6630 - acc: 0.6486 - val_loss: 2.3363 - val_acc: 0.7556\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.75556 to 0.76790, saving model to best_model2\n",
      "0s - loss: 2.4516 - acc: 0.7219 - val_loss: 2.2552 - val_acc: 0.7679\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc did not improve\n",
      "0s - loss: 2.2846 - acc: 0.7335 - val_loss: 2.2065 - val_acc: 0.7605\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.76790 to 0.78765, saving model to best_model2\n",
      "0s - loss: 2.2349 - acc: 0.7314 - val_loss: 2.1189 - val_acc: 0.7877\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "0s - loss: 2.1368 - acc: 0.7643 - val_loss: 2.0825 - val_acc: 0.7827\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc did not improve\n",
      "0s - loss: 2.0378 - acc: 0.7792 - val_loss: 2.0197 - val_acc: 0.7852\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc improved from 0.78765 to 0.79259, saving model to best_model2\n",
      "0s - loss: 1.9644 - acc: 0.7803 - val_loss: 1.9431 - val_acc: 0.7926\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc did not improve\n",
      "0s - loss: 1.9032 - acc: 0.8121 - val_loss: 1.9161 - val_acc: 0.7802\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc improved from 0.79259 to 0.79753, saving model to best_model2\n",
      "0s - loss: 1.7810 - acc: 0.8450 - val_loss: 1.8445 - val_acc: 0.7975\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc improved from 0.79753 to 0.80988, saving model to best_model2\n",
      "0s - loss: 1.7258 - acc: 0.8546 - val_loss: 1.7833 - val_acc: 0.8099\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "0s - loss: 1.6454 - acc: 0.8482 - val_loss: 1.7415 - val_acc: 0.8025\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc improved from 0.80988 to 0.82469, saving model to best_model2\n",
      "0s - loss: 1.5707 - acc: 0.8769 - val_loss: 1.6784 - val_acc: 0.8247\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc did not improve\n",
      "0s - loss: 1.5303 - acc: 0.8684 - val_loss: 1.6598 - val_acc: 0.7877\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc did not improve\n",
      "0s - loss: 1.4380 - acc: 0.8981 - val_loss: 1.5936 - val_acc: 0.8123\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "0s - loss: 1.3888 - acc: 0.8992 - val_loss: 1.5448 - val_acc: 0.8148\n",
      "Epoch 17/100\n",
      "Epoch 00016: val_acc did not improve\n",
      "0s - loss: 1.3264 - acc: 0.9098 - val_loss: 1.5075 - val_acc: 0.8148\n",
      "Epoch 18/100\n",
      "Epoch 00017: val_acc did not improve\n",
      "0s - loss: 1.2665 - acc: 0.9076 - val_loss: 1.4609 - val_acc: 0.8049\n",
      "Epoch 19/100\n",
      "Epoch 00018: val_acc did not improve\n",
      "0s - loss: 1.2165 - acc: 0.9225 - val_loss: 1.4540 - val_acc: 0.8049\n",
      "Epoch 20/100\n",
      "Epoch 00019: val_acc did not improve\n",
      "0s - loss: 1.1483 - acc: 0.9427 - val_loss: 1.3882 - val_acc: 0.8173\n",
      "Epoch 21/100\n",
      "Epoch 00020: val_acc did not improve\n",
      "0s - loss: 1.1260 - acc: 0.9374 - val_loss: 1.3595 - val_acc: 0.8148\n",
      "Epoch 22/100\n",
      "Epoch 00021: val_acc did not improve\n",
      "0s - loss: 1.0474 - acc: 0.9554 - val_loss: 1.3401 - val_acc: 0.8049\n",
      "Epoch 23/100\n",
      "Epoch 00022: val_acc did not improve\n",
      "0s - loss: 1.0208 - acc: 0.9395 - val_loss: 1.3024 - val_acc: 0.8148\n",
      "Epoch 24/100\n",
      "Epoch 00023: val_acc did not improve\n",
      "0s - loss: 0.9920 - acc: 0.9480 - val_loss: 1.2987 - val_acc: 0.8099\n",
      "Epoch 25/100\n",
      "Epoch 00024: val_acc did not improve\n",
      "0s - loss: 0.9371 - acc: 0.9607 - val_loss: 1.2495 - val_acc: 0.8148\n",
      "Epoch 26/100\n",
      "Epoch 00025: val_acc did not improve\n",
      "0s - loss: 0.9002 - acc: 0.9586 - val_loss: 1.2266 - val_acc: 0.8123\n",
      "Epoch 27/100\n",
      "Epoch 00026: val_acc did not improve\n",
      "0s - loss: 0.8649 - acc: 0.9628 - val_loss: 1.2165 - val_acc: 0.8049\n",
      "Epoch 28/100\n",
      "Epoch 00027: val_acc did not improve\n",
      "0s - loss: 0.8275 - acc: 0.9650 - val_loss: 1.1705 - val_acc: 0.8099\n",
      "Epoch 29/100\n",
      "Epoch 00028: val_acc did not improve\n",
      "0s - loss: 0.7855 - acc: 0.9756 - val_loss: 1.1479 - val_acc: 0.8173\n",
      "Epoch 30/100\n",
      "Epoch 00029: val_acc did not improve\n",
      "0s - loss: 0.7639 - acc: 0.9713 - val_loss: 1.1412 - val_acc: 0.8173\n",
      "Epoch 31/100\n",
      "Epoch 00030: val_acc did not improve\n",
      "0s - loss: 0.7384 - acc: 0.9682 - val_loss: 1.1098 - val_acc: 0.8198\n",
      "Epoch 32/100\n",
      "Epoch 00031: val_acc did not improve\n",
      "0s - loss: 0.7217 - acc: 0.9682 - val_loss: 1.0913 - val_acc: 0.8247\n",
      "Epoch 33/100\n",
      "Epoch 00032: val_acc did not improve\n",
      "0s - loss: 0.6730 - acc: 0.9777 - val_loss: 1.0915 - val_acc: 0.8099\n",
      "Epoch 00032: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fit sentiment model using pretrained word vectors\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "NUM_OUTPUT_UNITS_1=len(mlb2.classes_)\n",
    "\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# We use our 450 labeled sentiment reviews\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "MAX_DOC_LEN=30\n",
    "sequences_sentiment = tokenizer.texts_to_sequences(sentiment_l[\"Reviews\"])\n",
    "padded_sequences_sentiment = pad_sequences(sequences_sentiment, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "#sequences_2 = tokenizer.texts_to_sequences(predict[1])\n",
    "#padded_sequences_2 = pad_sequences(sequences_2, \\\n",
    "                                 #maxlen=MAX_DOC_LEN, \\\n",
    "                                 #padding='post', \\\n",
    "                                 #truncating='post')\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(\\\n",
    "                padded_sequences_sentiment[0:450], Y2[0:450], \\\n",
    "                test_size=0.3, random_state=0)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model_1=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, NUM_OUTPUT_UNITS_1, \\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping_1=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint_1 = ModelCheckpoint(BEST_MODEL_FILEPATH2, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training_1=model_1.fit(X_train_1, Y_train_1, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping_1, checkpoint_1],\\\n",
    "          validation_data=[X_test_1, Y_test_1], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 94.28%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of sentiment CNN model\n",
    "scores_2 = model_1.evaluate(padded_sequences_sentiment, Y2, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_1.metrics_names[1], scores_2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model configuration of aspect model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 30, 100)       200100      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                  (None, 29, 64)        12864       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                  (None, 28, 64)        19264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_4 (Conv1D)                  (None, 27, 64)        25664       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_2 (MaxPooling1D)             (None, 1, 64)         0           conv_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_3 (MaxPooling1D)             (None, 1, 64)         0           conv_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_4 (MaxPooling1D)             (None, 1, 64)         0           conv_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flat_2 (Flatten)                 (None, 64)            0           max_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_3 (Flatten)                 (None, 64)            0           max_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_4 (Flatten)                 (None, 64)            0           max_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_2[0][0]                     \n",
      "                                                                   flat_3[0][0]                     \n",
      "                                                                   flat_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 4)             772         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 295,720\n",
      "Trainable params: 95,620\n",
      "Non-trainable params: 200,100\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "model configuration of sentiment model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 30, 100)       200100      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                  (None, 29, 64)        12864       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                  (None, 28, 64)        19264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_4 (Conv1D)                  (None, 27, 64)        25664       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_2 (MaxPooling1D)             (None, 1, 64)         0           conv_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_3 (MaxPooling1D)             (None, 1, 64)         0           conv_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_4 (MaxPooling1D)             (None, 1, 64)         0           conv_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flat_2 (Flatten)                 (None, 64)            0           max_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_3 (Flatten)                 (None, 64)            0           max_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_4 (Flatten)                 (None, 64)            0           max_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_2[0][0]                     \n",
      "                                                                   flat_3[0][0]                     \n",
      "                                                                   flat_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 3)             579         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 295,527\n",
      "Trainable params: 95,427\n",
      "Non-trainable params: 200,100\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check model configuration\n",
    "print(\"model configuration of aspect model\")\n",
    "print(model.summary())\n",
    "print(\"\\n\")\n",
    "print(\"model configuration of sentiment model\")\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   ambience       1.00      0.82      0.90        66\n",
      "       food       0.91      0.98      0.94       259\n",
      "      price       1.00      0.46      0.63        37\n",
      "   serivice       0.93      0.91      0.92        94\n",
      "\n",
      "avg / total       0.93      0.90      0.91       456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation of aspect CNN\n",
    "# Let's use samples[0:400]\n",
    "# as an evaluation set\n",
    "from sklearn.metrics import classification_report\n",
    "pred_1=model.predict(padded_sequences_aspect[0:400])\n",
    "\n",
    "Y_pred_1=np.matrix(pred_1)\n",
    "Y_pred_1=np.where(Y_pred_1>0.5,1,0)\n",
    "Y1=np.array(Y1)\n",
    "\n",
    "Y_pred_1[0:10]\n",
    "Y1[100:110]\n",
    "\n",
    "#y = map(lambda x: int(x), y)\n",
    "#answer = map(lambda x: int(x), answer)\n",
    "print(classification_report(Y1[0:400], Y_pred_1, target_names=mlb1.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negative       0.92      0.91      0.92       148\n",
      "    Neutral       0.97      0.69      0.81        42\n",
      "   Positive       0.93      0.92      0.93       210\n",
      "\n",
      "avg / total       0.93      0.89      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation of sentiment CNN\n",
    "# Let's use samples[0:400]\n",
    "# as an evaluation set\n",
    "pred_2=model_1.predict(padded_sequences_sentiment[0:400])\n",
    "\n",
    "Y_pred_2=np.matrix(pred_2)\n",
    "Y_pred_2=np.where(Y_pred_2>0.5,1,0)\n",
    "#Y=np.array(Y)\n",
    "\n",
    "Y_pred_2[0:10]\n",
    "Y2[100:110]\n",
    "\n",
    "#y = map(lambda x: int(x), y)\n",
    "#answer = map(lambda x: int(x), answer)\n",
    "print(classification_report(Y2[0:400], Y_pred_2, target_names=mlb2.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambience' 'food' 'price' 'serivice']\n",
      "[[0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 1 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Predict the unlabelled dataset with aspects, and save the result into a csv file\n",
    "pred_11=model.predict(padded_sequences_1[0:10602])\n",
    "\n",
    "#we chose a threshold of 0.3, because at 30% probability, every review can have a label, and some aspects that \n",
    "#could be mentioned but not mentioned a lot in the review could be reflected\n",
    "Y_pred_11=np.matrix(pred_11)\n",
    "Y_pred_11=np.where(Y_pred_11>0.3,1,0)\n",
    "\n",
    "#name of each column\n",
    "print(mlb1.classes_)\n",
    "print(Y_pred_11[0:200])\n",
    "\n",
    "result_1 = pd.DataFrame(Y_pred_11)\n",
    "result_1.to_csv('output1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Predict the unlabelled dataset with sentiments, and save the result into a csv file\n",
    "pred_12=model_1.predict(padded_sequences_1[0:10602])\n",
    "Y_pred_12=np.matrix(pred_12)\n",
    "#print(Y_pred_12)\n",
    "#Because each review can only have a sentiment as a whole, we chose sentiment with the highest probability for each reveiw \n",
    "#as its representitive sentiment. And the highest probability is converted to 1, others 0.\n",
    "print((Y_pred_12 == Y_pred_12.max(axis=1)).astype(float))\n",
    "\n",
    "result_2 = pd.DataFrame((Y_pred_12 == Y_pred_12.max(axis=1)).astype(float))\n",
    "result_2.to_csv('output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
